import pandas as pd
import urllib.request
import numpy as np
import pandas as pd  # Importing the Pandas library

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
filename="iris.csv"
urllib.request.urlretrieve(url, filename)
print (f"Iris dataset downloaded and saved as {filename}.")
iris = pd.read_csv(filename, sep=',', decimal='.', header=None, names= ['sepal_length','sepal_width',
 'petal_length','petal_width',
 'target'])
#select all the lines which has sepal
#length greater than 6.
mask_feature = iris['sepal_length'] > 6.0

print(mask_feature)

mask_target = iris['target'] == 'Iris-virginica'
iris.loc[mask_target, 'target'] = 'New label'

print(iris['target'].unique())

grouped_targets_mean = iris.groupby(['target']).mean()

print(grouped_targets_mean)

grouped_targets_var = iris.groupby(['target']).var()

print(grouped_targets_var)

funcs = {'sepal_length':['mean','std'],
 'sepal_width' :['max', 'min'],
 'petal_length':['mean','std'],
 'petal_width' :['max', 'min']}
grouped_targets_f = iris.groupby(['target']).agg(funcs)

print(grouped_targets_f)
print(iris.sort_values(by='sepal_length').head())
print(iris.apply(np.count_nonzero, axis=1).head())
print(iris.apply(np.count_nonzero, axis=0))
print(iris.sum(axis=0))
print(iris.applymap(lambda x:len(str(x))).head())

def square(x):
 return x**2
original_variables = ['sepal_length', 'sepal_width',
 'petal_length', 'petal_width']
squared_iris = iris[original_variables].apply(square)

print(squared_iris)

# Reading the CSV file into a DataFrame
dataset = pd.read_csv("C://data_science/data_exploration_godlove_kuaban/Datasets files/a_selection_example_1.csv")
print(dataset)  # Printing the DataFrame

dataset = pd.read_csv("C://data_science/data_exploration_godlove_kuaban/Datasets files/a_selection_example_1.csv", index_col=0)

print(dataset)

print(dataset['val3'][104])
print(dataset.loc[104, 'val3'])
print(dataset.iloc[4, 2])
print(dataset[['val3', 'val2']][0:2])
print(dataset.loc[range(100, 102), ['val3', 'val2']])
print(dataset.iloc[range(2), [2,1]])
categorical_feature = pd.Series(['sunny', 'cloudy', 'snowy',
 'rainy', 'foggy'])
mapping = pd.get_dummies(categorical_feature)

print(mapping)
print(mapping['sunny'])
print(mapping['cloudy'])

#
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
#

le = LabelEncoder()
ohe = OneHotEncoder()
levels = ['sunny', 'cloudy', 'snowy', 'rainy', 'foggy']
fit_levs = le.fit_transform(levels)
ohe.fit([[fit_levs[0]], [fit_levs[1]], [fit_levs[2]], [fit_levs[3]],
[fit_levs[4]]])
print (ohe.transform([le.transform(['sunny'])]).toarray())
print (ohe.transform([le.transform(['cloudy'])]).toarray())

# ## CATEGORIES AND TEXTUAL DATA ## #

categorical_feature = pd.Series(['sunny', 'cloudy', 'snowy', 'rainy', 'foggy'])
mapping = pd.get_dummies(categorical_feature)
print(mapping)
print (mapping['sunny'])
print (mapping['cloudy'])

#
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
#

le = LabelEncoder()
ohe = OneHotEncoder()
levels = ['sunny', 'cloudy', 'snowy', 'rainy', 'foggy']
fit_levs = le.fit_transform(levels)
ohe.fit([[fit_levs[0]], [fit_levs[1]], [fit_levs[2]], [fit_levs[3]], [fit_levs[4]]])

print (ohe.transform([le.transform(['sunny'])]).toarray())

print (ohe.transform([le.transform(['cloudy'])]).toarray())

from sklearn.datasets import fetch_20newsgroups

categories = ['sci.med', 'sci.space']

twenty_sci_news = fetch_20newsgroups(categories=categories)

print(twenty_sci_news.data[0])
print(twenty_sci_news.filenames)
print (twenty_sci_news.target[0])
print (twenty_sci_news.target_names[twenty_sci_news.target[0]])
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
word_count = count_vect.fit_transform(twenty_sci_news.data)
print(word_count.shape)
print (word_count[0])

word_list = count_vect.get_feature_names_out()
for n in word_count[0].indices:
    print ('Word "%s" appears %i times' % (word_list[n], word_count[0, n]))

from sklearn.feature_extraction.text import TfidfVectorizer

tf_vect = TfidfVectorizer(use_idf=False, norm='l1')
word_freq = tf_vect.fit_transform(twenty_sci_news.data)
word_list = tf_vect.get_feature_names_out()

# Indented the print statement to be part of the for loop
for n in word_freq[0].indices:
    print('Word "%s" has frequency %0.3f' % (word_list[n], word_freq[0, n]))

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vect = TfidfVectorizer() # Default: use_idf=True
word_tfidf = tfidf_vect.fit_transform(twenty_sci_news.data)
word_list = tfidf_vect.get_feature_names_out()
for n in word_tfidf[0].indices:
 print('Word "%s" has tf-idf %0.3f' % (word_list[n], word_tfidf[0, n]))

text_1='welovedatascience'
text_2='datascienceishard'
documents=[text_1, text_2]
print(documents)

#
from sklearn.feature_extraction.text import CountVectorizer
#

#Define the documents
text_1='we love data science'
text_2='data science is hard'
documents=[text_1, text_2]

#That is what we say above,the defaul tone

count_vect_1_grams=CountVectorizer(ngram_range=(1,1),
stop_words=[],min_df=1)
word_count=count_vect_1_grams.fit_transform(documents)
word_list=count_vect_1_grams.get_feature_names_out()
print("Wordlist=",word_list)
print("text_1isdescribedwith",[word_list[n]+"("+
str(word_count[0,n])+")"for n in word_count[0].indices])

#Nowabi-gramcountvectorizer

count_vect_1_grams=CountVectorizer(ngram_range=(2,2))
word_count=count_vect_1_grams.fit_transform(documents)
word_list=count_vect_1_grams.get_feature_names_out()

print("Wordlist=",word_list)
print("text_1isdescribedwith",[word_list[n]+"("+

str(word_count[0,n])+")"for n in word_count[0].indices])

# Now a uni- and bi-gram count vectorizer

count_vect_1_grams = CountVectorizer(ngram_range=(1, 2))
word_count = count_vect_1_grams.fit_transform(documents)
word_list = count_vect_1_grams.get_feature_names_out()

print ("Word list = ", word_list)
print ("text_1 is described with", [word_list[n] + "(" +

str(word_count[0, n]) + ")" for n in word_count[0].indices])

from sklearn.datasets import fetch_20newsgroups
categories = ['sci.med', 'sci.space']
twenty_sci_news = fetch_20newsgroups(categories=categories)
from sklearn.feature_extraction.text import HashingVectorizer
hash_vect = HashingVectorizer(n_features=1000)
word_hashed = hash_vect.fit_transform(twenty_sci_news.data)

print(word_hashed.shape)
